{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- 1. Data Preparation ---\n# Define transformations for training and testing\n# Data augmentation for training to improve generalization\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n])\n\n# Load CIFAR-10 datasets\ntrainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntestset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\n# Create DataLoaders\nbatch_size = 128\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\ntestloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n# --- 2. Define Models with Attention Mechanism ---\n\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channels):\n        super(SelfAttention, self).__init__()\n        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        batch_size, C, H, W = x.size()\n        \n        # Reshape to (batch_size, channels, H*W) and permute for matrix multiplication\n        proj_query = self.query_conv(x).view(batch_size, -1, H*W).permute(0, 2, 1) # B, HW, C'\n        proj_key = self.key_conv(x).view(batch_size, -1, H*W) # B, C', HW\n        \n        energy = torch.bmm(proj_query, proj_key) # B, HW, HW (attention map)\n        attention = torch.softmax(energy, dim=-1)\n        \n        proj_value = self.value_conv(x).view(batch_size, -1, H*W) # B, C, HW\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1)) # B, C, HW\n        out = out.view(batch_size, C, H, W)\n        \n        out = self.gamma * out + x\n        return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, use_attention=False):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n        \n        self.use_attention = use_attention\n        if use_attention:\n            self.attention = SelfAttention(self.expansion * planes)\n\n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = torch.relu(out)\n        if self.use_attention:\n            out = self.attention(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10, use_attention=False):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n        self.use_attention = use_attention\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride, self.use_attention))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = nn.functional.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\ndef ResNet18_Attention():\n    return ResNet(BasicBlock, [2, 2, 2, 2], use_attention=True)\n\ndef ResNet18_NoAttention():\n    return ResNet(BasicBlock, [2, 2, 2, 2], use_attention=False)\n\n# --- 3. Ensemble Learning ---\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super(EnsembleModel, self).__init__()\n        self.models = nn.ModuleList(models)\n    \n    def forward(self, x):\n        # Collect predictions from each model\n        outputs = [model(x) for model in self.models]\n        # Average the predictions (soft voting)\n        avg_output = torch.mean(torch.stack(outputs, dim=0), dim=0)\n        return avg_output\n\n# --- 4. Training Function ---\ndef train_model(model, dataloader, optimizer, criterion, epoch, model_name=\"Model\"):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(dataloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n    avg_loss = running_loss / len(dataloader)\n    accuracy = 100. * correct / total\n    print(f'Epoch {epoch} | {model_name} Loss: {avg_loss:.4f} | Acc: {accuracy:.2f}%')\n    return avg_loss, accuracy\n\n# --- 5. Evaluation Function ---\ndef evaluate_model(model, dataloader, criterion, model_name=\"Model\"):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_targets = []\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    avg_loss = running_loss / len(dataloader)\n    accuracy = 100. * correct / total\n    precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='macro', zero_division=0)\n    \n    print(f'\\n{model_name} Test Loss: {avg_loss:.4f} | Test Acc: {accuracy:.2f}%')\n    print(f'{model_name} Test Precision: {precision:.4f} | Test Recall: {recall:.4f} | Test F1-Score: {f1:.4f}\\n')\n    return accuracy, precision, recall, f1\n\n# --- Main Training and Evaluation Loop ---\nif __name__ == \"__main__\":\n    num_epochs = 20 # You might want to increase this for better performance\n\n    # Initialize individual models\n    print(\"Initializing individual models...\")\n    model1 = ResNet18_NoAttention().to(device)\n    model2 = ResNet18_Attention().to(device)\n    model3 = ResNet18_NoAttention().to(device) # Another instance for diversity\n    model4 = ResNet18_Attention().to(device)  # Another instance with attention\n\n    individual_models = [model1, model2, model3, model4]\n    model_names = [\"ResNet18_NoAttention_1\", \"ResNet18_Attention_1\", \"ResNet18_NoAttention_2\", \"ResNet18_Attention_2\"]\n\n    criterions = [nn.CrossEntropyLoss() for _ in individual_models]\n    optimizers = [optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) for model in individual_models]\n    schedulers = [optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs) for optimizer in optimizers]\n\n    # Train individual models\n    print(\"\\n--- Training Individual Models ---\")\n    for i, model in enumerate(individual_models):\n        print(f\"\\nTraining {model_names[i]}:\")\n        best_acc = 0\n        for epoch in range(num_epochs):\n            train_loss, train_acc = train_model(model, trainloader, optimizers[i], criterions[i], epoch + 1, model_names[i])\n            schedulers[i].step() # Update learning rate\n\n            # Save best model\n            if train_acc > best_acc:\n                best_acc = train_acc\n                torch.save(model.state_dict(), f'{model_names[i]}_best.pth')\n                print(f\"  Saved best model for {model_names[i]} with accuracy: {best_acc:.2f}%\")\n\n    # Load best weights for individual models\n    print(\"\\nLoading best weights for individual models...\")\n    for i, model in enumerate(individual_models):\n        model.load_state_dict(torch.load(f'{model_names[i]}_best.pth'))\n        model.eval() # Set to evaluation mode\n\n    # Create and evaluate the ensemble model\n    print(\"\\n--- Evaluating Ensemble Model ---\")\n    ensemble_model = EnsembleModel(individual_models).to(device)\n    ensemble_criterion = nn.CrossEntropyLoss() # Only needed for evaluation loss calculation\n    \n    # Evaluate individual models on the test set\n    print(\"\\n--- Individual Model Test Performance ---\")\n    for i, model in enumerate(individual_models):\n        print(f\"Evaluating {model_names[i]}:\")\n        evaluate_model(model, testloader, ensemble_criterion, model_names[i])\n\n    # Evaluate the ensemble model\n    print(\"\\n--- Ensemble Model Test Performance ---\")\n    ensemble_accuracy, ensemble_precision, ensemble_recall, ensemble_f1 = \\\n        evaluate_model(ensemble_model, testloader, ensemble_criterion, \"Ensemble_Model\")\n    \n    print(\"\\n--- Final Results ---\")\n    print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.2f}%\")\n    print(f\"Ensemble Model Precision: {ensemble_precision:.4f}\")\n    print(f\"Ensemble Model Recall: {ensemble_recall:.4f}\")\n    print(f\"Ensemble Model F1-Score: {ensemble_f1:.4f}\")\n\n    # You can also visualize some predictions if desired\n    def visualize_predictions(model, test_loader, classes, num_images=5):\n        model.eval()\n        dataiter = iter(test_loader)\n        images, labels = next(dataiter)\n\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n\n        print('\\n--- Visualizing Predictions ---')\n        plt.figure(figsize=(10, 2))\n        for i in range(num_images):\n            plt.subplot(1, num_images, i + 1)\n            img = images[i].cpu().permute(1, 2, 0).numpy()\n            img = img / 2 + 0.5  # Unnormalize for display\n            plt.imshow(img.clip(0, 1)) # Clip to ensure valid RGB values\n            plt.title(f'True: {classes[labels[i]]}\\nPred: {classes[predicted[i]]}')\n            plt.axis('off')\n        plt.show()\n\n    # visualize_predictions(ensemble_model, testloader, classes, num_images=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T06:50:15.860852Z","iopub.execute_input":"2025-06-27T06:50:15.861500Z","iopub.status.idle":"2025-06-27T08:28:52.371149Z","shell.execute_reply.started":"2025-06-27T06:50:15.861465Z","shell.execute_reply":"2025-06-27T08:28:52.370145Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:02<00:00, 61.3MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Initializing individual models...\n\n--- Training Individual Models ---\n\nTraining ResNet18_NoAttention_1:\nEpoch 1 | ResNet18_NoAttention_1 Loss: 1.4366 | Acc: 47.67%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 47.67%\nEpoch 2 | ResNet18_NoAttention_1 Loss: 0.9369 | Acc: 66.85%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 66.85%\nEpoch 3 | ResNet18_NoAttention_1 Loss: 0.7168 | Acc: 74.83%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 74.83%\nEpoch 4 | ResNet18_NoAttention_1 Loss: 0.5968 | Acc: 79.29%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 79.29%\nEpoch 5 | ResNet18_NoAttention_1 Loss: 0.5163 | Acc: 82.12%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 82.12%\nEpoch 6 | ResNet18_NoAttention_1 Loss: 0.4472 | Acc: 84.56%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 84.56%\nEpoch 7 | ResNet18_NoAttention_1 Loss: 0.4026 | Acc: 86.01%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 86.01%\nEpoch 8 | ResNet18_NoAttention_1 Loss: 0.3620 | Acc: 87.34%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 87.34%\nEpoch 9 | ResNet18_NoAttention_1 Loss: 0.3226 | Acc: 88.81%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 88.81%\nEpoch 10 | ResNet18_NoAttention_1 Loss: 0.2919 | Acc: 89.86%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 89.86%\nEpoch 11 | ResNet18_NoAttention_1 Loss: 0.2542 | Acc: 91.13%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 91.13%\nEpoch 12 | ResNet18_NoAttention_1 Loss: 0.2313 | Acc: 92.04%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 92.04%\nEpoch 13 | ResNet18_NoAttention_1 Loss: 0.1955 | Acc: 93.22%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 93.22%\nEpoch 14 | ResNet18_NoAttention_1 Loss: 0.1728 | Acc: 94.00%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 94.00%\nEpoch 15 | ResNet18_NoAttention_1 Loss: 0.1541 | Acc: 94.75%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 94.75%\nEpoch 16 | ResNet18_NoAttention_1 Loss: 0.1326 | Acc: 95.44%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 95.44%\nEpoch 17 | ResNet18_NoAttention_1 Loss: 0.1144 | Acc: 96.15%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 96.15%\nEpoch 18 | ResNet18_NoAttention_1 Loss: 0.1051 | Acc: 96.53%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 96.53%\nEpoch 19 | ResNet18_NoAttention_1 Loss: 0.0973 | Acc: 96.84%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 96.84%\nEpoch 20 | ResNet18_NoAttention_1 Loss: 0.0924 | Acc: 97.03%\n  Saved best model for ResNet18_NoAttention_1 with accuracy: 97.03%\n\nTraining ResNet18_Attention_1:\nEpoch 1 | ResNet18_Attention_1 Loss: 1.4120 | Acc: 48.46%\n  Saved best model for ResNet18_Attention_1 with accuracy: 48.46%\nEpoch 2 | ResNet18_Attention_1 Loss: 0.8989 | Acc: 68.22%\n  Saved best model for ResNet18_Attention_1 with accuracy: 68.22%\nEpoch 3 | ResNet18_Attention_1 Loss: 0.6885 | Acc: 75.99%\n  Saved best model for ResNet18_Attention_1 with accuracy: 75.99%\nEpoch 4 | ResNet18_Attention_1 Loss: 0.5743 | Acc: 80.08%\n  Saved best model for ResNet18_Attention_1 with accuracy: 80.08%\nEpoch 5 | ResNet18_Attention_1 Loss: 0.4955 | Acc: 82.89%\n  Saved best model for ResNet18_Attention_1 with accuracy: 82.89%\nEpoch 6 | ResNet18_Attention_1 Loss: 0.4367 | Acc: 84.86%\n  Saved best model for ResNet18_Attention_1 with accuracy: 84.86%\nEpoch 7 | ResNet18_Attention_1 Loss: 0.3897 | Acc: 86.38%\n  Saved best model for ResNet18_Attention_1 with accuracy: 86.38%\nEpoch 8 | ResNet18_Attention_1 Loss: 0.3452 | Acc: 87.96%\n  Saved best model for ResNet18_Attention_1 with accuracy: 87.96%\nEpoch 9 | ResNet18_Attention_1 Loss: 0.3083 | Acc: 89.31%\n  Saved best model for ResNet18_Attention_1 with accuracy: 89.31%\nEpoch 10 | ResNet18_Attention_1 Loss: 0.2738 | Acc: 90.47%\n  Saved best model for ResNet18_Attention_1 with accuracy: 90.47%\nEpoch 11 | ResNet18_Attention_1 Loss: 0.2452 | Acc: 91.48%\n  Saved best model for ResNet18_Attention_1 with accuracy: 91.48%\nEpoch 12 | ResNet18_Attention_1 Loss: 0.2135 | Acc: 92.63%\n  Saved best model for ResNet18_Attention_1 with accuracy: 92.63%\nEpoch 13 | ResNet18_Attention_1 Loss: 0.1873 | Acc: 93.44%\n  Saved best model for ResNet18_Attention_1 with accuracy: 93.44%\nEpoch 14 | ResNet18_Attention_1 Loss: 0.1610 | Acc: 94.48%\n  Saved best model for ResNet18_Attention_1 with accuracy: 94.48%\nEpoch 15 | ResNet18_Attention_1 Loss: 0.1398 | Acc: 95.27%\n  Saved best model for ResNet18_Attention_1 with accuracy: 95.27%\nEpoch 16 | ResNet18_Attention_1 Loss: 0.1174 | Acc: 95.98%\n  Saved best model for ResNet18_Attention_1 with accuracy: 95.98%\nEpoch 17 | ResNet18_Attention_1 Loss: 0.1017 | Acc: 96.39%\n  Saved best model for ResNet18_Attention_1 with accuracy: 96.39%\nEpoch 18 | ResNet18_Attention_1 Loss: 0.0881 | Acc: 97.06%\n  Saved best model for ResNet18_Attention_1 with accuracy: 97.06%\nEpoch 19 | ResNet18_Attention_1 Loss: 0.0807 | Acc: 97.19%\n  Saved best model for ResNet18_Attention_1 with accuracy: 97.19%\nEpoch 20 | ResNet18_Attention_1 Loss: 0.0761 | Acc: 97.47%\n  Saved best model for ResNet18_Attention_1 with accuracy: 97.47%\n\nTraining ResNet18_NoAttention_2:\nEpoch 1 | ResNet18_NoAttention_2 Loss: 1.4405 | Acc: 47.42%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 47.42%\nEpoch 2 | ResNet18_NoAttention_2 Loss: 0.9019 | Acc: 67.98%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 67.98%\nEpoch 3 | ResNet18_NoAttention_2 Loss: 0.7011 | Acc: 75.61%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 75.61%\nEpoch 4 | ResNet18_NoAttention_2 Loss: 0.5819 | Acc: 79.81%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 79.81%\nEpoch 5 | ResNet18_NoAttention_2 Loss: 0.5057 | Acc: 82.42%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 82.42%\nEpoch 6 | ResNet18_NoAttention_2 Loss: 0.4441 | Acc: 84.56%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 84.56%\nEpoch 7 | ResNet18_NoAttention_2 Loss: 0.3946 | Acc: 86.29%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 86.29%\nEpoch 8 | ResNet18_NoAttention_2 Loss: 0.3532 | Acc: 87.70%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 87.70%\nEpoch 9 | ResNet18_NoAttention_2 Loss: 0.3143 | Acc: 89.00%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 89.00%\nEpoch 10 | ResNet18_NoAttention_2 Loss: 0.2876 | Acc: 90.08%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 90.08%\nEpoch 11 | ResNet18_NoAttention_2 Loss: 0.2556 | Acc: 91.09%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 91.09%\nEpoch 12 | ResNet18_NoAttention_2 Loss: 0.2243 | Acc: 92.16%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 92.16%\nEpoch 13 | ResNet18_NoAttention_2 Loss: 0.1974 | Acc: 93.12%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 93.12%\nEpoch 14 | ResNet18_NoAttention_2 Loss: 0.1726 | Acc: 94.07%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 94.07%\nEpoch 15 | ResNet18_NoAttention_2 Loss: 0.1482 | Acc: 94.99%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 94.99%\nEpoch 16 | ResNet18_NoAttention_2 Loss: 0.1301 | Acc: 95.67%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 95.67%\nEpoch 17 | ResNet18_NoAttention_2 Loss: 0.1146 | Acc: 96.11%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 96.11%\nEpoch 18 | ResNet18_NoAttention_2 Loss: 0.1026 | Acc: 96.53%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 96.53%\nEpoch 19 | ResNet18_NoAttention_2 Loss: 0.0945 | Acc: 96.90%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 96.90%\nEpoch 20 | ResNet18_NoAttention_2 Loss: 0.0899 | Acc: 97.08%\n  Saved best model for ResNet18_NoAttention_2 with accuracy: 97.08%\n\nTraining ResNet18_Attention_2:\nEpoch 1 | ResNet18_Attention_2 Loss: 1.4248 | Acc: 48.02%\n  Saved best model for ResNet18_Attention_2 with accuracy: 48.02%\nEpoch 2 | ResNet18_Attention_2 Loss: 0.8790 | Acc: 68.95%\n  Saved best model for ResNet18_Attention_2 with accuracy: 68.95%\nEpoch 3 | ResNet18_Attention_2 Loss: 0.6811 | Acc: 76.39%\n  Saved best model for ResNet18_Attention_2 with accuracy: 76.39%\nEpoch 4 | ResNet18_Attention_2 Loss: 0.5675 | Acc: 80.28%\n  Saved best model for ResNet18_Attention_2 with accuracy: 80.28%\nEpoch 5 | ResNet18_Attention_2 Loss: 0.4965 | Acc: 82.70%\n  Saved best model for ResNet18_Attention_2 with accuracy: 82.70%\nEpoch 6 | ResNet18_Attention_2 Loss: 0.4393 | Acc: 84.76%\n  Saved best model for ResNet18_Attention_2 with accuracy: 84.76%\nEpoch 7 | ResNet18_Attention_2 Loss: 0.3909 | Acc: 86.51%\n  Saved best model for ResNet18_Attention_2 with accuracy: 86.51%\nEpoch 8 | ResNet18_Attention_2 Loss: 0.3489 | Acc: 87.91%\n  Saved best model for ResNet18_Attention_2 with accuracy: 87.91%\nEpoch 9 | ResNet18_Attention_2 Loss: 0.3132 | Acc: 89.04%\n  Saved best model for ResNet18_Attention_2 with accuracy: 89.04%\nEpoch 10 | ResNet18_Attention_2 Loss: 0.2803 | Acc: 90.34%\n  Saved best model for ResNet18_Attention_2 with accuracy: 90.34%\nEpoch 11 | ResNet18_Attention_2 Loss: 0.2497 | Acc: 91.31%\n  Saved best model for ResNet18_Attention_2 with accuracy: 91.31%\nEpoch 12 | ResNet18_Attention_2 Loss: 0.2219 | Acc: 92.21%\n  Saved best model for ResNet18_Attention_2 with accuracy: 92.21%\nEpoch 13 | ResNet18_Attention_2 Loss: 0.1965 | Acc: 93.23%\n  Saved best model for ResNet18_Attention_2 with accuracy: 93.23%\nEpoch 14 | ResNet18_Attention_2 Loss: 0.1685 | Acc: 94.24%\n  Saved best model for ResNet18_Attention_2 with accuracy: 94.24%\nEpoch 15 | ResNet18_Attention_2 Loss: 0.1448 | Acc: 94.95%\n  Saved best model for ResNet18_Attention_2 with accuracy: 94.95%\nEpoch 16 | ResNet18_Attention_2 Loss: 0.1225 | Acc: 95.76%\n  Saved best model for ResNet18_Attention_2 with accuracy: 95.76%\nEpoch 17 | ResNet18_Attention_2 Loss: 0.1108 | Acc: 96.16%\n  Saved best model for ResNet18_Attention_2 with accuracy: 96.16%\nEpoch 18 | ResNet18_Attention_2 Loss: 0.0950 | Acc: 96.79%\n  Saved best model for ResNet18_Attention_2 with accuracy: 96.79%\nEpoch 19 | ResNet18_Attention_2 Loss: 0.0868 | Acc: 97.03%\n  Saved best model for ResNet18_Attention_2 with accuracy: 97.03%\nEpoch 20 | ResNet18_Attention_2 Loss: 0.0840 | Acc: 97.27%\n  Saved best model for ResNet18_Attention_2 with accuracy: 97.27%\n\nLoading best weights for individual models...\n\n--- Evaluating Ensemble Model ---\n\n--- Individual Model Test Performance ---\nEvaluating ResNet18_NoAttention_1:\n\nResNet18_NoAttention_1 Test Loss: 0.2878 | Test Acc: 91.00%\nResNet18_NoAttention_1 Test Precision: 0.9101 | Test Recall: 0.9100 | Test F1-Score: 0.9100\n\nEvaluating ResNet18_Attention_1:\n\nResNet18_Attention_1 Test Loss: 0.3054 | Test Acc: 91.44%\nResNet18_Attention_1 Test Precision: 0.9144 | Test Recall: 0.9144 | Test F1-Score: 0.9143\n\nEvaluating ResNet18_NoAttention_2:\n\nResNet18_NoAttention_2 Test Loss: 0.2847 | Test Acc: 91.11%\nResNet18_NoAttention_2 Test Precision: 0.9110 | Test Recall: 0.9111 | Test F1-Score: 0.9110\n\nEvaluating ResNet18_Attention_2:\n\nResNet18_Attention_2 Test Loss: 0.2964 | Test Acc: 91.22%\nResNet18_Attention_2 Test Precision: 0.9121 | Test Recall: 0.9122 | Test F1-Score: 0.9121\n\n\n--- Ensemble Model Test Performance ---\n\nEnsemble_Model Test Loss: 0.2375 | Test Acc: 92.49%\nEnsemble_Model Test Precision: 0.9248 | Test Recall: 0.9249 | Test F1-Score: 0.9248\n\n\n--- Final Results ---\nEnsemble Model Accuracy: 92.49%\nEnsemble Model Precision: 0.9248\nEnsemble Model Recall: 0.9249\nEnsemble Model F1-Score: 0.9248\n","output_type":"stream"}],"execution_count":1}]}